{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis: Impact of Review Scores on Price\n",
    "\n",
    "Null Hypothesis (H0): There is no relationship between review scores and listing price.\n",
    "\n",
    "Alternative Hypothesis (H1): Higher review scores lead to higher listing prices.\n",
    "\n",
    "EDA Approach: Calculate correlations between review scores and price. Fit a multiple linear regression model (e.g., price ~ cleanliness + location_score + communication_score).\n",
    "\n",
    "In this experiment, we aim to investigate the relationship between review scores and listing prices in the context of a rental property marketplace. The overarching question we seek to address is whether higher review scores, encompassing factors such as cleanliness, location, and communication correspond to higher listing prices. \n",
    "\n",
    "Our null hypothesis (H0) posits that review scores do not significantly affect listing prices. While the alternative hypothesis (H1) suggests that higher review scores lead to higher listing prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis (EDA)\n",
    "\n",
    "To explore the relationship between review scores and listing prices, we employ various techniques aimed at understanding the data distribution and identifying correlations. \n",
    "\n",
    "These techniques include:\n",
    "\n",
    "Calculating Correlations: We compute correlations between review scores (e.g., cleanliness, location, communication) and listing prices. This helps us understand the strength and direction of the relationship between these variables.\n",
    "\n",
    "Fitting a Multiple Linear Regression Model: We fit a multiple linear regression model, where the listing price is the dependent variable, and review scores are the independent variables. This model allows us to quantify the impact of each review score on the listing price while controlling for other factors.\n",
    "\n",
    "Calculating Spearman’s Rank Correlation: Spearman’s rank correlation coefficient is a statistical method used to determine the strength and direction of the relationship between two ordinal variables (ordinal refers to categorical data with a specific order or ranking). In our analysis, review scores (e.g., cleanliness and location,) are examples of ordinal variables because they can be ranked from lowest to highest (variables). It’s particularly useful when dealing with ordinal data, where the actual numerical differences between values may not be meaningful. This method ensures robustness against outliers and non-linear relationships; providing reliable insights into the association between variables in our analysis.\n",
    "\n",
    "Calculating Summary Statistics: We compute summary statistics, including mean, median, standard deviation, minimum, maximum, and quartiles for both review scores and listing prices. These statistics provide insights into the central tendency and variability of the data.\n",
    "\n",
    "\n",
    "Visualizations: We create various visualizations, including box plots, scatter plots, and histograms to further explore the distribution and relationships within the data. Box plots help us visualize the distribution of review scores and their relationship with listing prices. Scatter plots provide insights into the linear relationship between review scores and listing prices. While histograms offer a visual representation of the distribution of review scores and listing prices individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell is for loading the relevant libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd              # For data manipulation\n",
    "import numpy as np               # For numerical operations\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt # For plotting graphs\n",
    "import seaborn as sns            # For enhanced visualization\n",
    "from sklearn.model_selection import train_test_split  # For splitting data into training and testing sets\n",
    "from sklearn.linear_model import LinearRegression    # For linear regression modeling\n",
    "from sklearn.metrics import mean_squared_error, r2_score  # For model evaluation metrics\n",
    "\n",
    "# Set backend for matplotlib to TkAgg for interactive plotting\n",
    "# Note: This line may not be necessary if you are already using an interactive backend\n",
    "matplotlib.use('TkAgg')\n",
    "\n",
    "# Specify the file path\n",
    "file = '/Users/olisa/Downloads/clean_df.csv'  # Update file path as on your computer\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "data = pd.read_csv(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TkAgg\n"
     ]
    }
   ],
   "source": [
    "# Checking whether the interactive backend has been set right\n",
    "\n",
    "print(matplotlib.get_backend())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is cleaning the dataframe and removing the colums that are not neccsary for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of relevant columns for analysis\n",
    "relevant_columns = ['price', 'review_scores_rating', 'review_scores_accuracy', \n",
    "                    'review_scores_cleanliness', 'review_scores_checkin', \n",
    "                    'review_scores_communication', 'review_scores_location',\n",
    "                    'review_scores_value']\n",
    "\n",
    "# Creating a subset of the original DataFrame with only the relevant columns\n",
    "subset_data = data[relevant_columns]\n",
    "\n",
    "# Display the first 5 rows of the new DataFrame\n",
    "print(subset_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check and potentially handle any missing values and convert any datatypes if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the data types of columns in the subset DataFrame\n",
    "print(subset_data.dtypes)\n",
    "\n",
    "# Print the number of missing values in each column of the subset DataFrame\n",
    "print(subset_data.isna().sum(axis=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have confirmed that there are some missing values so we will need to fix that. In addition price is in the wrong type so lets convert it to a numeric one. After that we will proceed with some descriptive statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As there are a significant amount of missing values for the review score we will have to 'drop' these rows from the dataframe entirely.\n",
    "\n",
    "subset_data = subset_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove commas from the 'price' column\n",
    "subset_data['price'] = subset_data['price'].str.replace(',', '')\n",
    "\n",
    "# Remove dollar sign ('$') from the 'price' column\n",
    "subset_data['price'] = subset_data['price'].str.replace('$', '')\n",
    "\n",
    "# Convert the 'price' column to float datatype\n",
    "subset_data['price'] = subset_data['price'].astype(float)\n",
    "\n",
    "# Print the 'price' column and its datatype after conversion\n",
    "print(subset_data['price'])\n",
    "print(subset_data['price'].dtypes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking that we have succesfully cleaned the data\n",
    "\n",
    "# Check if all data types are float64\n",
    "if subset_data.dtypes.eq('float64').all():\n",
    "    # Check if there are no missing values\n",
    "    if subset_data.isna().sum().sum() == 0:\n",
    "        print(\"Data successfully cleaned.\")\n",
    "    else:\n",
    "        print(\"Error: Missing values detected after cleaning.\")\n",
    "else:\n",
    "    print(\"Error: Data types not successfully converted to float64.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics for the 'price' column\n",
    "\n",
    "# Mean\n",
    "mean_price = subset_data['price'].mean()\n",
    "\n",
    "# Median\n",
    "median_price = subset_data['price'].median()\n",
    "\n",
    "# Standard Deviation\n",
    "std_price = subset_data['price'].std()\n",
    "\n",
    "# Variance\n",
    "var_price = subset_data['price'].var()\n",
    "\n",
    "# Minimum and Maximum\n",
    "min_price = subset_data['price'].min()\n",
    "max_price = subset_data['price'].max()\n",
    "\n",
    "# Quartiles\n",
    "q25_price = subset_data['price'].quantile(0.25)\n",
    "q75_price = subset_data['price'].quantile(0.75)\n",
    "\n",
    "# Store summary statistics in a list\n",
    "summ_stats = [(f'mean = {mean_price}'), (f'median = {median_price}'), \n",
    "              (f'standard deviation = {std_price}'), (f'variance = {var_price}'), \n",
    "              (f'min = {min_price}'), (f'max = {max_price}'), \n",
    "              (f'q25 = {q25_price}'), (f'q75 = {q75_price}')]\n",
    "\n",
    "# Print each summary statistic\n",
    "for stat in summ_stats:\n",
    "    print(stat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean: The mean listing price is $157.24, indicating the average price of listings in the dataset.\n",
    "\n",
    "Median: The median listing price is $99.00. This suggests that half of the listings have prices below $99.00 and half have prices above $99.00.\n",
    "\n",
    "Standard Deviation: The standard deviation of $296.79 tells us how much each listing price deviates, on average, from the mean price of $157.24., the standard deviation is relatively high, thus it means that listing prices vary widely around the average, indicating a broader range of prices.\n",
    "\n",
    "Variance: The variance of $88083.60, being the square of the standard deviation, provides a measure of the overall variability or spread of listing prices from the mean. A high variance suggests that listing prices are widely spread out from the mean, indicating a diverse range of prices within the dataset\n",
    "\n",
    "Minimum: The minimum listing price is $0.00, which suggests the presence of listings with no cost or possibly outliers.\n",
    "\n",
    "Maximum: The maximum listing price is $23000.00, indicating the highest price in the dataset.\n",
    "\n",
    "25th Percentile (Q1): The 25th percentile, is $56.00. This means that 25% of listings have prices below $56.00.\n",
    "\n",
    "75th Percentile (Q3): The 75th percentile,  is $171.00. This means that 75% of listings have prices below $171.00.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These stats on there own are not very useful so lets do some data visualisation work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boxplot - A boxplot provides a concise summary of the distribution of the data, including measures such as the median, quartiles, and potential outliers. Here's how you can create a boxplot for the 'price' variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boxplot to visualize the distribution of prices\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot the boxplot\n",
    "sns.boxplot(x='price', data=subset_data, palette='pastel')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Boxplot of Price')\n",
    "plt.xlabel('Price')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatter plots vizualize the relationship between review score and price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plots to visualize the relationship between each review score and the listing price\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Creating a list of review score columns\n",
    "review_scores = ['review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness', \n",
    "                 'review_scores_checkin', 'review_scores_communication', 'review_scores_location',\n",
    "                 'review_scores_value']\n",
    "\n",
    "# Iterate over each review score and create a scatter plot\n",
    "for score in review_scores:\n",
    "    sns.scatterplot(x=score, y='price', data=subset_data, palette='pastel')\n",
    "    plt.xlabel(f'Review Score ({score.replace(\"review_scores_\", \"\")})')\n",
    "    plt.ylabel('Listing Price')\n",
    "    plt.title(f'Impact of {score} on Listing Price')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histograms show us the frequency distribution of the given variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histograms to visualize the distribution of each review score\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Creating a list of review score columns\n",
    "review_scores = ['review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness', \n",
    "                 'review_scores_checkin', 'review_scores_communication', 'review_scores_location',\n",
    "                 'review_scores_value']\n",
    "\n",
    "# Iterate over each review score and create a histogram\n",
    "for score in review_scores:\n",
    "    sns.histplot(x=score, data=subset_data, bins=10, palette='pastel')\n",
    "    plt.xlabel(f'Review Score ({score.replace(\"review_scores_\", \"\")})')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(f'Distribution of {score}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter graphs reveal a moderately strong correlation between review scores and listing prices. This suggests that higher review scores are associated with higher listing prices, supporting the hypothesis that better-reviewed properties tend to command higher prices.\n",
    "\n",
    "One valuable insight from the histograms is the distribution of review scores among the properties. The predominance of ratings at 4 and 5 stars indicates that the majority of properties receive high ratings from guests. This could suggests that the properties listed in your dataset generally provide satisfactory experiences for guests, which could positively influence their pricing and overall desirability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation analysis - using Spearmans rank\n",
    "Spearman's rank correlation is a way to see if there's a pattern between these two factors (in this case it will be price and a review score)\n",
    "The stronger the correlation (closer to 1 or -1), the more closely review scores and prices are connected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary library for Spearman's rank correlation\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Dictionary to store correlation coefficients and p-values for each review score\n",
    "correlation_results = {}\n",
    "\n",
    "# Iterating over each review score variable\n",
    "for score in review_scores:\n",
    "    # Calculating Spearman's rank correlation coefficient between the review score and prices\n",
    "    spearman_corr, p_value = spearmanr(subset_data[score], subset_data['price'])\n",
    "    \n",
    "    # Storing correlation coefficient and p-value in the dictionary\n",
    "    correlation_results[score] = {'correlation_coefficient': spearman_corr, 'p_value': p_value}\n",
    "\n",
    "# Printing the results\n",
    "for score, result in correlation_results.items():\n",
    "    print(f\"Spearman's Rank Correlation for {score}:\")\n",
    "    print(\"Correlation Coefficient:\", result['correlation_coefficient'])\n",
    "    print(\"P-value:\", result['p_value'])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Spearman's rank correlation coefficients using a heatmap\n",
    "\n",
    "# Extract correlation coefficients from the dictionary\n",
    "correlation_coefficients = [[correlation_results[score]['correlation_coefficient'] for score in review_scores]]\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Create a heatmap\n",
    "sns.heatmap(correlation_coefficients, annot=True, cmap='coolwarm', xticklabels=review_scores, yticklabels=['Price'], fmt=\".2f\")\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Spearman's Rank Correlation Coefficients between Review Scores and Price\")\n",
    "plt.xlabel('Review Scores')\n",
    "plt.ylabel('Price')\n",
    "\n",
    "# Display the heatmap\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation Coefficient: This value indicates the strength and direction of the relationship between review scores and listing prices. A correlation coefficient closer to 1 or -1 suggests a strong positive or negative correlation, respectively, while a value closer to 0 indicates a weaker correlation. For example, a correlation coefficient of -0.016 for review_scores_rating suggests a very weak negative correlation between rating scores and listing prices.\n",
    "\n",
    "P-value: This tells us the probability of observing such a correlation coefficient by random chance alone, assuming the null hypothesis is true (i.e., no correlation). A small p-value (typically below 0.05) indicates that the observed correlation is unlikely to be due to random chance, suggesting that there is a significant relationship between the variables. For instance, a p-value of 0.00033 for review_scores_rating suggests a significant correlation between rating scores and listing prices.\n",
    "\n",
    "Review Scores Rating: There is a very weak negative correlation between review scores rating and listing prices, but it is statistically significant.\n",
    "\n",
    "Review Scores Accuracy: There is a weak negative correlation between accuracy scores and listing prices, and it is statistically significant.\n",
    "\n",
    "Review Scores Cleanliness: There is a weak positive correlation between cleanliness scores and listing prices, and it is statistically significant.\n",
    "\n",
    "Review Scores Check-in: There is a moderate negative correlation between check-in scores and listing prices, and it is statistically significant.\n",
    "\n",
    "Review Scores Communication: There is a moderate negative correlation between communication scores and listing prices, and it is statistically significant.\n",
    "\n",
    "Review Scores Location: There is a strong positive correlation between location scores and listing prices, and it is statistically significant.\n",
    "\n",
    "Review Scores Value: There is a strong negative correlation between value scores and listing prices, and it is statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the Spearman's rank correlation results we will omit certain features. We do this to fine tune the linear regression model and for other benefits such as: Improved interpretability, Simplification of the model :\n",
    "\n",
    "Review Scores Rating: This feature has a very weak negative correlation with listing prices and a relatively high p-value (0.00033). While statistically significant, the correlation coefficient is close to zero, suggesting a negligible effect on listing prices.\n",
    "\n",
    "Review Scores Accuracy: Similar to the rating scores, accuracy scores exhibit a weak negative correlation with listing prices but with a statistically significant p-value (4.79e-24). However, the correlation coefficient is relatively small (-0.044), indicating a minor impact on prices.\n",
    "\n",
    "Review Scores Cleanliness: While cleanliness scores show a weak positive correlation with listing prices, the correlation coefficient (0.018) is small, and the p-value (2.83e-05) is statistically significant. Considering the weak correlation, this feature may not significantly contribute to predicting prices.\n",
    "\n",
    "Review Scores Communication: Communication scores demonstrate a moderate negative correlation with listing prices, but with a statistically significant p-value (6.12e-46). However, the correlation coefficient (-0.063) suggests a relatively modest influence on prices compared to other features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features (X) and target variable (y)\n",
    "features = ['review_scores_checkin', 'review_scores_location', 'review_scores_value']\n",
    "X = subset_data[features]\n",
    "y = subset_data['price']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse:.2f}')\n",
    "print(f'R-squared: {r2:.2f}')\n",
    "\n",
    "# Interpretation (coefficients)\n",
    "coefficients = pd.DataFrame({'Feature': X.columns, 'Coefficient': model.coef_})\n",
    "print(coefficients)\n",
    "\n",
    "# Set Seaborn color palette to pastel\n",
    "sns.set_palette(\"pastel\")\n",
    "\n",
    "# Visualization (actual vs. predicted)\n",
    "sns.scatterplot(x=y_test, y=y_pred, alpha=0.5)\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')\n",
    "plt.xlabel('Actual Price')\n",
    "plt.ylabel('Predicted Price')\n",
    "plt.title('Regression Model: Actual vs. Predicted Price')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear regression model aimed to predict listing prices based on review scores, including cleanliness, location, value, check-in experience, communication, and overall rating. However, the model's performance was poor, with a high mean squared error (MSE) of 92082.93 and a low R-squared value of 0.01, indicating limited predictive accuracy. Coefficients revealed minimal impact of review scores on listing prices, suggesting the need for model refinement. Lets try again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By minimizing the RMSE, we are aiming to improve the accuracy of our model's predictions regarding listing prices.\n",
    "Minimizing RMSE helps achieve this by fine-tuning the model's parameters to better capture the relationships between review scores and listing prices in the dataset. Ultimately, a lower RMSE indicates that the model's predictions are closer to the actual listing prices, making it more reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "# Define objective function to minimize (RMSE)\n",
    "def rmse_objective(params):\n",
    "    # Fit linear regression model with given parameters\n",
    "    model = LinearRegression()\n",
    "    X_train_subset = X_train.drop(columns=['review_scores_checkin', 'review_scores_value'])\n",
    "    model.fit(X_train_subset, y_train)\n",
    "    \n",
    "    # Make predictions on test set\n",
    "    X_test_subset = X_test.drop(columns=['review_scores_checkin', 'review_scores_value'])\n",
    "    y_pred = model.predict(X_test_subset)\n",
    "    \n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    return rmse\n",
    "\n",
    "# Set initial guess for parameters\n",
    "initial_guess = [1.0] * len(X_train.columns)\n",
    "\n",
    "# Run optimization\n",
    "optimized_params = minimize(rmse_objective, initial_guess, method='Nelder-Mead')\n",
    "\n",
    "# Extract optimized parameters\n",
    "optimized_features = optimized_params.x\n",
    "\n",
    "# Fit linear regression model with optimized parameters\n",
    "model = LinearRegression()\n",
    "X_train_subset = X_train.drop(columns=['review_scores_checkin', 'review_scores_value'])\n",
    "model.fit(X_train_subset, y_train)\n",
    "\n",
    "# Make predictions on test set\n",
    "X_test_subset = X_test.drop(columns=['review_scores_checkin', 'review_scores_value'])\n",
    "y_pred = model.predict(X_test_subset)\n",
    "\n",
    "# Evaluate model performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Interpretation (coefficients)\n",
    "coefficients = pd.DataFrame({'Feature': X_train_subset.columns, 'Coefficient': model.coef_})\n",
    "\n",
    "# Visualization (actual vs. predicted)\n",
    "sns.scatterplot(x=y_test, y=y_pred, alpha=0.5)\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')\n",
    "plt.xlabel('Actual Price')\n",
    "plt.ylabel('Predicted Price')\n",
    "plt.title('Regression Model: Actual vs. Predicted Price')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of this project, where we aimed to analyze the impact of review scores on listing prices, the scatter plot showing the disparity between actual and predicted prices highlights a crucial issue. It suggests that our current multiple linear regression model, despite incorporating various review scores as features, fails to accurately predict listing prices. This discrepancy could potentially invalidate our hypothesis regarding the influence of review scores on prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, our analysis focused on investigating the impact of review scores on listing prices in the context of our project. We began by formulating hypotheses, with the null hypothesis stating that review scores do not significantly affect listing prices, and the alternative hypothesis proposing that higher review scores lead to higher prices. Through exploratory data analysis (EDA), we employed various techniques such as correlation analysis, linear regression modeling, and visualization to examine the relationship between review scores and prices.\n",
    "\n",
    "Our EDA revealed several key findings. Firstly, Spearman's rank correlation analysis indicated significant correlations between certain review scores (e.g., location, cleanliness) and listing prices, suggesting a potential influence of these factors on pricing decisions. However, the linear regression model yielded less promising results, with a high mean squared error (MSE) and low R-squared value, indicating poor model fit and predictive performance. Despite attempts to optimize the model using techniques like minimizing root mean squared error (RMSE), the model's predictions remained inconsistent with the actual prices.\n",
    "\n",
    "Furthermore, visualizations such as box plots, scatter plots, and histograms provided additional insights into the distribution and relationship of review scores and prices. While scatter plots exhibited moderately strong correlations between review scores and prices, the box plots and histograms did not offer significant value due to data skewness and lack of variability in certain variables.\n",
    "\n",
    "In conclusion, while our analysis provided some evidence supporting the influence of review scores on listing prices, the predictive capabilities of our models were limited. Our findings corroborate the null hypothesis, indicating that review scores may not have a significant impact on listing prices. Future research could explore alternative modeling techniques, feature engineering, or additional variables to improve the accuracy and robustness of price prediction models in the vacation rental industry. Additionally, incorporating qualitative factors such as property amenities, location characteristics, and guest reviews could offer a more comprehensive understanding of pricing dynamics in the market."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
